{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7035cc55",
   "metadata": {},
   "source": [
    "## New ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaefb600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fade84a8130>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kolyangg/anaconda3/envs/photomaker/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "/home/kolyangg/anaconda3/envs/photomaker/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/kolyangg/anaconda3/envs/photomaker/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.\n",
      "  deprecate(\"Transformer2DModelOutput\", \"1.0.0\", deprecation_message)\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:03<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PhotoMaker v2 components [1] id_encoder from [/home/kolyangg/.cache/huggingface/hub/models--TencentARC--PhotoMaker-V2/snapshots/f5a1e5155dc02166253fa7e29d13519f5ba22eac]...\n",
      "4096\n",
      "Loading PhotoMaker v2 components [2] lora_weights from [/home/kolyangg/.cache/huggingface/hub/models--TencentARC--PhotoMaker-V2/snapshots/f5a1e5155dc02166253fa7e29d13519f5ba22eac]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kolyangg/anaconda3/envs/photomaker/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:121: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "model ignore: /home/kolyangg/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "model ignore: /home/kolyangg/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/kolyangg/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "model ignore: /home/kolyangg/.insightface/models/buffalo_l/genderage.onnx genderage\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/kolyangg/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n",
      "[DEBUG] face token positions in aux prompt: [1]\n",
      "[DEBUG] ‖v_face‖ = 33.25\n",
      "prompt: a girl img in the arctic wearing a red jacket\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kolyangg/anaconda3/envs/photomaker/lib/python3.10/site-packages/photomaker/pipeline.py:565: FutureWarning: `callback` is deprecated and will be removed in version 1.0.0. Passing `callback` as an input argument to `__call__` is deprecated, consider use `callback_on_step_end`\n",
      "  deprecate(\n",
      "/home/kolyangg/anaconda3/envs/photomaker/lib/python3.10/site-packages/photomaker/pipeline.py:571: FutureWarning: `callback_steps` is deprecated and will be removed in version 1.0.0. Passing `callback_steps` as an input argument to `__call__` is deprecated, consider use `callback_on_step_end`\n",
      "  deprecate(\n",
      "  2%|▏         | 1/50 [00:00<00:20,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] first-snapshot  layer=down_blocks.1.attentions.0.transformer_blocks.0.attn2  max=0.6250  mean=0.4044\n",
      "[VAL0] max@(63,0)=0.625  centre=0.426  corner=0.539\n",
      "[VAL] centre=0.426  corner=0.539  max=0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_210800/2752509750.py:274: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  colormap  = cm.get_cmap(cmap_name)\n",
      "100%|██████████| 50/50 [00:32<00:00,  1.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# If required libraries are not installed, uncomment the following:\n",
    "# !pip install opencv-python transformers accelerate\n",
    "\n",
    "\n",
    "import os, math\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.cm as cm\n",
    "from diffusers.utils import load_image\n",
    "from diffusers import EulerDiscreteScheduler\n",
    "from photomaker import PhotoMakerStableDiffusionXLPipeline, FaceAnalysis2, analyze_faces\n",
    "from transformers import CLIPTokenizer            # ← add (needed later if you keep tokenizer logic)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from pathlib import Path\n",
    "import types, math\n",
    "\n",
    "\n",
    "# ── global font object (about 3 % of the tile height) ───────────────────\n",
    "FONT_SIZE = 40                                  # tweak if tiles change\n",
    "try:                                            # try a TTF first\n",
    "    ttf_path = next(\n",
    "        p for p in (\n",
    "            Path(\"/usr/share/fonts\"), Path(\"/usr/local/share/fonts\"))\n",
    "        if p.is_dir()).rglob(\"DejaVuSans.ttf\").__next__()\n",
    "    font = ImageFont.truetype(str(ttf_path), FONT_SIZE)\n",
    "except StopIteration:\n",
    "    font = ImageFont.load_default()             # fallback bitmap font\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Device and dtype setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "torch_dtype = torch.bfloat16 if (device == \"cuda\" and torch.cuda.is_bf16_supported()) else torch.float16\n",
    "if device == \"mps\":\n",
    "    torch_dtype = torch.float16\n",
    "\n",
    "# Load base SDXL model and PhotoMaker V2 adapter:contentReference[oaicite:4]{index=4}:contentReference[oaicite:5]{index=5}\n",
    "pipe = PhotoMakerStableDiffusionXLPipeline.from_pretrained(\n",
    "    \"SG161222/RealVisXL_V4.0\", torch_dtype=torch_dtype\n",
    ").to(device)\n",
    "from huggingface_hub import hf_hub_download\n",
    "ckpt_path = hf_hub_download(repo_id=\"TencentARC/PhotoMaker-V2\", filename=\"photomaker-v2.bin\", repo_type=\"model\")\n",
    "pipe.load_photomaker_adapter(os.path.dirname(ckpt_path), subfolder=\"\", weight_name=os.path.basename(ckpt_path), trigger_word=\"img\")\n",
    "pipe.fuse_lora()\n",
    "pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)  # Euler sampler:contentReference[oaicite:6]{index=6}\n",
    "pipe.disable_xformers_memory_efficient_attention()  # disable for attention inspection\n",
    "\n",
    "# Reference image and identity embedding preparation\n",
    "# reference_image_path = \"keanu.jpg\"  # <--- set your reference image path\n",
    "# reference_image_path = \"tom.jpg\" \n",
    "# reference_image_path = \"eddie.webp\" \n",
    "reference_image_path = \"sydney.jpg\" \n",
    "\n",
    "\n",
    "# Which token should the heat-map follow?\n",
    "#   \"face\" – fixed auxiliary prompt \"a face\" (sharper ID-agnostic face map)\n",
    "#   \"img\"  – the PhotoMaker trigger word inside *your* prompt\n",
    "#   \"man\"  – a normal word inside *your* prompt\n",
    "TOKEN_FOCUS = \"face\"            # ← change to \"img\" or \"man\" when needed\n",
    "# TOKEN_FOCUS = \"man\"  \n",
    "# prompt = \"a man img with a beard in a space shuttle\"\n",
    "# prompt = \"a portrait of a man img with a beard playing football\"\n",
    "# prompt = \"a man img with enjoying pasta in a restaurant\"\n",
    "prompt = \"a girl img in the arctic wearing a red jacket\"\n",
    "\n",
    "\n",
    "# Load reference image\n",
    "ref_image = load_image(reference_image_path)\n",
    "# Detect face and get identity embedding:contentReference[oaicite:7]{index=7}\n",
    "face_detector = FaceAnalysis2(providers=['CUDAExecutionProvider'], allowed_modules=['detection', 'recognition'])\n",
    "face_detector.prepare(ctx_id=0, det_size=(640, 640))\n",
    "img_np = np.array(ref_image)[:, :, ::-1]  # convert PIL (RGB) to BGR NumPy for detector\n",
    "faces = analyze_faces(face_detector, img_np)\n",
    "if not faces:\n",
    "    raise RuntimeError(\"No face detected in the reference image.\")\n",
    "\n",
    "id_embed = torch.from_numpy(faces[0][\"embedding\"]).unsqueeze(0)  # identity embedding tensor\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "tokenizer = getattr(pipe, \"tokenizer\", None) or CLIPTokenizer.from_pretrained(\n",
    "    \"SG161222/RealVisXL_V4.0\", subfolder=\"tokenizer\")\n",
    "\n",
    "\n",
    "if TOKEN_FOCUS == \"face\":\n",
    "    AUX_PROMPT = \"a face\"\n",
    "    with torch.no_grad():\n",
    "        face_latents, *_ = pipe.encode_prompt(\n",
    "            prompt=AUX_PROMPT, device=device,\n",
    "            num_images_per_prompt=1, do_classifier_free_guidance=False)   # (1,77,2048)\n",
    "\n",
    "    # ---- locate exact BPE sequence for “ face” -------------------------\n",
    "    aux_ids  = tokenizer(AUX_PROMPT, add_special_tokens=False).input_ids\n",
    "    face_ids = tokenizer(\" face\",    add_special_tokens=False).input_ids\n",
    "    # face_ids = tokenizer(\" wtf\",    add_special_tokens=False).input_ids\n",
    "    def find_sub(seq, sub):\n",
    "        for i in range(len(seq) - len(sub) + 1):\n",
    "            if seq[i:i+len(sub)] == sub:\n",
    "                return list(range(i, i+len(sub)))\n",
    "        return []\n",
    "    FACE_TOKEN_IDX = find_sub(aux_ids, face_ids)\n",
    "    if not FACE_TOKEN_IDX:\n",
    "        raise RuntimeError(\"Could not locate 'face' tokens in aux prompt\")\n",
    "    # debug\n",
    "    print(\"[DEBUG] face token positions in aux prompt:\", FACE_TOKEN_IDX)\n",
    "    print(\"[DEBUG] ‖v_face‖ =\",\n",
    "          face_latents[0, FACE_TOKEN_IDX].mean(0).norm().item())\n",
    "\n",
    "# Prepare lists to collect overlay images\n",
    "\n",
    "layer_names    = []     # to be filled after first callback\n",
    "heatmaps_cross = {}     # dict[layer] -> list[PIL.Image]\n",
    "final_image    = None\n",
    "\n",
    "\n",
    "# Seed for reproducibility\n",
    "seed = 56789\n",
    "generator = torch.Generator(device=device).manual_seed(seed)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 🪄  Monkey‑patch every CrossAttention: Q = to_q(hidden_states),\n",
    "#      K = to_k(face_latents); heat‑map = softmax(Q·K_face)\n",
    "###############################################################################\n",
    "from diffusers.models.attention_processor import Attention as CrossAttention\n",
    "\n",
    "attn_maps_current = {}                 # {layer_name: [head_maps]}\n",
    " \n",
    "\n",
    "\n",
    "def make_hook(layer_name, module):\n",
    "    orig_forward = module.forward\n",
    "    # scale = math.sqrt(module.head_dim)      # = √d  (matches SD-XL impl.)\n",
    "\n",
    "    def forward_with_hook(hidden_states,\n",
    "                          encoder_hidden_states=None,\n",
    "                          attention_mask=None):\n",
    "\n",
    "        # standard forward pass first ─ we still need the layer output\n",
    "        out = orig_forward(hidden_states, encoder_hidden_states, attention_mask)\n",
    "        if encoder_hidden_states is None:           # self-attention → skip\n",
    "            return out\n",
    "\n",
    "        # ── 0. keep only the “guided / conditional” half (CFG doubles batch) ──\n",
    "        B_all = hidden_states.shape[0]\n",
    "        hs_cond   = hidden_states[B_all // 2:]          # (B,L,C_img)\n",
    "        enc_cond  = encoder_hidden_states[B_all // 2:]  # (B,T,C_txt)\n",
    "\n",
    "        # ── 1. projections exactly like the real layer does ──────────────────\n",
    "        proj_q = (module.to_q if hasattr(module,\"to_q\") else module.q_proj)(hs_cond)\n",
    "        proj_k = (module.to_k if hasattr(module,\"to_k\") else module.k_proj)(\n",
    "                    enc_cond if TOKEN_FOCUS!=\"face\" else\n",
    "                    face_latents.to(hs_cond.dtype).repeat(hs_cond.shape[0],1,1))\n",
    "\n",
    "        B, L, C = proj_q.shape\n",
    "        h, d    = module.heads, C // module.heads\n",
    "\n",
    "        Q = proj_q.view(B, L, h, d).permute(0,2,1,3)        # (B,h,L,d)\n",
    "        K = proj_k.view(B, -1, h, d).permute(0,2,1,3)       # (B,h,T,d)\n",
    "\n",
    "        # ── 2. find token indices that spell TOKEN_FOCUS ─────────────────────\n",
    "        if TOKEN_FOCUS == \"face\":\n",
    "            token_idx = FACE_TOKEN_IDX          # list[int] from earlier\n",
    "        else:\n",
    "            prompt_ids = tokenizer(prompt,\n",
    "                                   add_special_tokens=False).input_ids\n",
    "            word_ids   = tokenizer(\" \"+TOKEN_FOCUS,\n",
    "                                   add_special_tokens=False).input_ids\n",
    "            # locate subsequence once\n",
    "            def find_sub(seq, sub):\n",
    "                for i in range(len(seq)-len(sub)+1):\n",
    "                    if seq[i:i+len(sub)] == sub:\n",
    "                        return list(range(i,i+len(sub)))\n",
    "                return []\n",
    "            token_idx = find_sub(prompt_ids, word_ids)\n",
    "            if not token_idx:\n",
    "                raise RuntimeError(f'\"{TOKEN_FOCUS}\" not found in prompt')\n",
    "\n",
    "        # ── 3. scaled dot-product *before* the soft-max (same as SD-XL) ──────\n",
    "        #     module.scale == 1/√dim_head\n",
    "        logits   = (Q @ K.transpose(-2, -1)) * module.scale   # (B,h,L,T)\n",
    "        weights  = logits.softmax(-1)                       # soft-max over *T*\n",
    "\n",
    "        # pick our token(s) and mean-pool if the word split into several BPEs\n",
    "        att      = weights[..., token_idx].mean(-1).mean(1)[0]   # (L,)\n",
    "\n",
    "        # ── 4. reshape to (H,W) and store ────────────────────────────────────\n",
    "        H = int(math.sqrt(att.numel()))\n",
    "        W = att.numel() // H\n",
    "\n",
    "        # NumPy has no native bfloat16 → cast first, then move to CPU\n",
    "        att2d = (\n",
    "           att.to(dtype=torch.float32)    # <─ NEW\n",
    "               .view(H, W)\n",
    "               .cpu()\n",
    "               .numpy()\n",
    "        )\n",
    "\n",
    "        layer_buf = attn_maps_current.setdefault(layer_name, [])\n",
    "        layer_buf.append(att2d)          # keep heads already averaged\n",
    "\n",
    "        return out\n",
    "\n",
    "    # assign plain function – works as instance attribute\n",
    "    return forward_with_hook\n",
    "\n",
    "\n",
    "\n",
    "for lname, mod in pipe.unet.named_modules():\n",
    "    if isinstance(mod, CrossAttention):\n",
    "        mod.forward = make_hook(lname, mod)\n",
    "\n",
    "\n",
    "num_steps = 50\n",
    "\n",
    "def callback(step, timestep, latents):\n",
    "    global attn_maps_current, layer_names, heatmaps_cross, final_image\n",
    "\n",
    "    if step % 10 == 0 or step == num_steps - 1:\n",
    "        # ❶  Consolidate maps *layer by layer* into square grids\n",
    "        snapshot = {}\n",
    "        for layer, maps in attn_maps_current.items():\n",
    "            flat  = np.stack(maps).mean(0)                 # mean over heads\n",
    "            H     = int(math.sqrt(flat.size))\n",
    "            snapshot[layer] = flat.reshape(H, H)\n",
    "            \n",
    "\n",
    "        if step == 0:\n",
    "            k, v = next(iter(snapshot.items()))\n",
    "            print(f\"[DEBUG] first-snapshot  layer={k}  max={v.max():.4f}  mean={v.mean():.4f}\")\n",
    "            \n",
    "            # NEW: print where the absolute max lives and some refs\n",
    "            flat_idx = v.argmax()\n",
    "            r, c     = divmod(flat_idx, v.shape[1])\n",
    "            print(f\"[VAL0] max@({r},{c})={v.max():.3f}  \"\n",
    "                  f\"centre={v[v.shape[0]//2, v.shape[1]//2]:.3f}  \"\n",
    "                  f\"corner={v[0,0]:.3f}\")\n",
    "\n",
    "\n",
    "        \n",
    "        # ── DEBUG: check numeric values before colouring ────────────────\n",
    "        if step == 0:\n",
    "            first_layer = next(iter(snapshot))\n",
    "            H0, W0      = snapshot[first_layer].shape\n",
    "            centre_val  = snapshot[first_layer][H0 // 2, W0 // 2]\n",
    "            corner_val  = snapshot[first_layer][0, 0]\n",
    "            print(f\"[VAL] centre={centre_val:.3f}  corner={corner_val:.3f}  \"\n",
    "                  f\"max={snapshot[first_layer].max():.3f}\")\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            vae_dev = next(pipe.vae.parameters()).device\n",
    "            img = pipe.vae.decode(\n",
    "                (latents / 0.18215).to(device=vae_dev, dtype=pipe.vae.dtype)\n",
    "            ).sample[0]\n",
    "\n",
    "        img_np = ((img.float() / 2 + 0.5).clamp(0, 1).cpu().permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
    "\n",
    "        if not layer_names:\n",
    "            layer_names = list(snapshot.keys())\n",
    "            heatmaps_cross = {ln: [] for ln in layer_names}\n",
    "\n",
    "        # jet = cm.get_cmap(\"jet\")\n",
    "        # pick colour-map: \"jet\" (default) or \"Greys\" for a monotone ramp\n",
    "        cmap_name = \"jet\"          # ← change to \"Greys\" if you like\n",
    "        INVERT    = False        # ← flip to True to check if colours are inverted\n",
    "        colormap  = cm.get_cmap(cmap_name)\n",
    "\n",
    "        # save the colour-bar once (step-0 of first layer)\n",
    "        if step == 0:\n",
    "            import matplotlib.pyplot as plt, numpy as _np\n",
    "            plt.figure(figsize=(4, .4))\n",
    "            plt.axis(\"off\")\n",
    "            plt.imshow(_np.linspace(0, 1, 256)[None, :],\n",
    "                       cmap=colormap, aspect=\"auto\")\n",
    "            plt.savefig(\"colourbar.png\", bbox_inches=\"tight\")\n",
    "            plt.close()        \n",
    "        \n",
    "        \n",
    "        for ln in layer_names:\n",
    "            amap = snapshot[ln]\n",
    "            # amap = (amap / amap.max()) if amap.max() > 0 else amap\n",
    "            \n",
    "            # # normalise 0‥1 and (optionally) invert\n",
    "            # amap = (amap / amap.max()) if amap.max() > 0 else amap\n",
    "            \n",
    "            # 1-step normalisation (0…1) – no *double* divide\n",
    "            amap = (amap / amap.max()) if amap.max() > 0 else amap\n",
    "            \n",
    "            if INVERT:\n",
    "                amap = 1.0 - amap\n",
    "            \n",
    "            \n",
    "            # hmap = (jet(amap)[..., :3] * 255).astype(np.uint8)\n",
    "            hmap = (colormap(amap)[..., :3] * 255).astype(np.uint8)\n",
    "            hmap = np.array(Image.fromarray(hmap).resize((1024, 1024),\n",
    "                                                         Image.BILINEAR))\n",
    "            # overlay = Image.fromarray((0.5 * img_np + 0.5 * hmap)\n",
    "            #                           .astype(np.uint8))\n",
    "            \n",
    "            # ───────────────────────── add 5×5 numerical grid ──────────────────\n",
    "            overlay_arr = (0.5 * img_np + 0.5 * hmap).astype(np.uint8)\n",
    "            overlay     = Image.fromarray(overlay_arr)\n",
    "            draw        = ImageDraw.Draw(overlay)\n",
    "\n",
    "            # split original similarity map (H×W) into 5×5 blocks\n",
    "            H_blk, W_blk = amap.shape[0] // 5, amap.shape[1] // 5\n",
    "            vis_h, vis_w = 1024 // 5, 1024 // 5              # overlay size\n",
    "\n",
    "            for bi in range(5):\n",
    "                for bj in range(5):\n",
    "                    block = amap[\n",
    "                        bi*H_blk : (bi+1)*H_blk,\n",
    "                        bj*W_blk : (bj+1)*W_blk]\n",
    "                    mean_val = block.mean()\n",
    "\n",
    "                    # position text at block centre (resize factor already 1024/H)\n",
    "                    cx = bj * vis_w + vis_w // 2\n",
    "                    cy = bi * vis_h + vis_h // 2\n",
    "\n",
    "                    txt = f\"{mean_val:.2f}\"\n",
    "                    tw, th = draw.textbbox((0, 0), txt, font=font)[2:]\n",
    "                    draw.text((cx - tw // 2, cy - th // 2),\n",
    "                              txt, font=font, fill=\"white\",\n",
    "                              stroke_width=2, stroke_fill=\"black\")\n",
    "            \n",
    "            \n",
    "            heatmaps_cross[ln].append(overlay)\n",
    "\n",
    "        # keep latest clean image for the “Final” panel\n",
    "        final_image = Image.fromarray(img_np)\n",
    "\n",
    "    attn_maps_current = {}      # reset per step\n",
    "\n",
    "# Run the diffusion process with the callback (50 steps):contentReference[oaicite:8]{index=8}\n",
    "\n",
    "print(f'prompt: {prompt}')\n",
    "\n",
    "_ = pipe(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=\"(asymmetry, worst quality, low quality, illustration, 3d, cartoon, sketch)\", \n",
    "    input_id_images=[ref_image], id_embeds=id_embed, \n",
    "    num_inference_steps=num_steps, \n",
    "    start_merge_step=10,\n",
    "    # start_merge_step=0, \n",
    "    generator=generator, callback=callback, callback_steps=1\n",
    ")\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "#  Build and save montage from the overlays collected in‑callback\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "import re\n",
    "header_h = 30\n",
    "\n",
    "for ln in layer_names:                          # set during callback\n",
    "    cols = []\n",
    "    for i, attn_img in enumerate(heatmaps_cross[ln]):\n",
    "        step_num = i * 10                       # 0,10,20,30,40,50\n",
    "        cols.append((f\"S{step_num}\", attn_img))\n",
    "\n",
    "    cols.append((\"Final\", final_image))         # last clean frame\n",
    "\n",
    "    img_w, img_h = cols[0][1].width, cols[0][1].height\n",
    "    strip = Image.new(\"RGB\", (img_w * len(cols), img_h + header_h),\n",
    "                      color=(0, 0, 0))\n",
    "    draw  = ImageDraw.Draw(strip)\n",
    "    font  = ImageFont.load_default()\n",
    "\n",
    "    for idx, (label, img) in enumerate(cols):\n",
    "        x_off = idx * img_w\n",
    "        strip.paste(img, (x_off, header_h))\n",
    "        tw, th = draw.textbbox((0, 0), label, font=font)[2:]\n",
    "        draw.text((x_off + (img_w - tw)//2, (header_h - th)//2),\n",
    "                  label, font=font, fill=(255, 255, 255))\n",
    "\n",
    "    safe = re.sub(r\"[^\\w\\-]+\", \"_\", ln)\n",
    "    dir = Path(\"heatmaps\")\n",
    "    dir.mkdir(exist_ok=True)\n",
    "    strip.save(dir / f\"{safe}_attn_hm.jpg\")\n",
    "\n",
    "# save final image\n",
    "if final_image is not None:\n",
    "    final_image.save(\"final_image.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f06d4472",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ─────────────────────────────────────────────────────────────\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#  QUICK PIL‑ONLY PDF MAKER  (≤10 rows per portrait A4 page)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ─────────────────────────────────────────────────────────────\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image, ImageDraw, ImageFont\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# --- config --------------------------------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/photomaker/lib/python3.10/site-packages/PIL/__init__.py:18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Pillow (Fork of the Python Imaging Library)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mPillow is the friendly PIL fork by Jeffrey A. Clark and contributors.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m;-)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _version\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# VERSION was removed in Pillow 6.0.0.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# PILLOW_VERSION was removed in Pillow 9.0.0.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Use __version__ instead.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m _version\u001b[38;5;241m.\u001b[39m__version__\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:975\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1074\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "#  QUICK PIL‑ONLY PDF MAKER  (≤10 rows per portrait A4 page)\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os\n",
    "\n",
    "# --- config --------------------------------------------------\n",
    "DPI         = 150                       # output resolution\n",
    "PAGE_W_PX   = int(8.27 * DPI)           # A4 portrait 8.27×11.69 in\n",
    "PAGE_H_PX   = int(11.69 * DPI)\n",
    "ROWS_PER_PG = 10\n",
    "ROW_H_PX    = PAGE_H_PX // ROWS_PER_PG\n",
    "LABEL_W_PX  = int(PAGE_W_PX * 0.15)     # 15 % gutter for filename\n",
    "RIGHT_PAD   = 20                        # px margin on right\n",
    "FONT        = ImageFont.load_default()\n",
    "\n",
    "dir = Path(\"heatmaps\")\n",
    "if not dir.is_dir():\n",
    "    raise RuntimeError(f\"Directory {dir} not found. \"\n",
    "                       \"Run the attention evolution script first.\")\n",
    "\n",
    "# gather montage strips\n",
    "montage_files = sorted(\n",
    "    f for f in os.listdir(dir) if f.endswith('__attn_hm.jpg')\n",
    ")\n",
    "\n",
    "pages, y = [], 0\n",
    "page = Image.new('RGB', (PAGE_W_PX, PAGE_H_PX), 'white')\n",
    "draw = ImageDraw.Draw(page)\n",
    "\n",
    "def wrapped_label(draw, text, x, y_top, row_h, max_w):\n",
    "    \"\"\"Draw *any* filename (no spaces needed) within max_w pixels.\"\"\"\n",
    "    line_h  = FONT.getbbox(\"A\")[3]\n",
    "    max_lin = row_h // line_h\n",
    "    lines, cur = [], \"\"\n",
    "\n",
    "    for ch in text:\n",
    "        trial = cur + ch\n",
    "        if draw.textlength(trial, font=FONT) <= max_w:\n",
    "            cur = trial\n",
    "        else:\n",
    "            lines.append(cur)\n",
    "            cur = ch\n",
    "    lines.append(cur)\n",
    "\n",
    "    if len(lines) > max_lin:                 # truncate vertically\n",
    "        lines = lines[:max_lin]\n",
    "        if len(lines[-1]) > 1:\n",
    "            while draw.textlength(lines[-1] + \"…\", font=FONT) > max_w:\n",
    "                lines[-1] = lines[-1][:-1]\n",
    "            lines[-1] += \"…\"\n",
    "\n",
    "    y_txt = y_top + (row_h - line_h * len(lines)) // 2\n",
    "    for ln in lines:\n",
    "        draw.text((x, y_txt), ln, fill=\"black\", font=FONT)\n",
    "        y_txt += line_h\n",
    "\n",
    "\n",
    "for fname in montage_files:\n",
    "    # --- scale montage to fit row height *and* available width ----\n",
    "    strip = Image.open(fname)\n",
    "    max_w = PAGE_W_PX - LABEL_W_PX - RIGHT_PAD\n",
    "    scale = min(ROW_H_PX / strip.height, max_w / strip.width)\n",
    "    strip = strip.resize((int(strip.width * scale),\n",
    "                          int(strip.height * scale)),\n",
    "                         Image.LANCZOS)\n",
    "\n",
    "    # --- new page if needed ---------------------------------------\n",
    "    if y + ROW_H_PX > PAGE_H_PX:\n",
    "        pages.append(page)\n",
    "        page = Image.new('RGB', (PAGE_W_PX, PAGE_H_PX), 'white')\n",
    "        draw = ImageDraw.Draw(page)\n",
    "        y = 0\n",
    "\n",
    "    # --- filename (wrapped) ---------------------------------------\n",
    "    wrapped_label(draw, fname, 10, y, ROW_H_PX, LABEL_W_PX - 20)\n",
    "\n",
    "    # --- paste montage strip --------------------------------------\n",
    "    x_strip = LABEL_W_PX\n",
    "    y_strip = y + (ROW_H_PX - strip.height) // 2\n",
    "    page.paste(strip, (x_strip, y_strip))\n",
    "\n",
    "    y += ROW_H_PX\n",
    "\n",
    "# final page\n",
    "pages.append(page)\n",
    "\n",
    "# --- save multipage PDF -------------------------------------------\n",
    "dir = Path(\"hm_results\")\n",
    "dir.mkdir(exist_ok=True)\n",
    "out_pdf = dir / 'attn_hm.pdf'\n",
    "pages[0].save(out_pdf, save_all=True, append_images=pages[1:])\n",
    "print(f'PDF saved to {out_pdf}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7a95112",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpipe\u001b[49m\u001b[38;5;241m.\u001b[39munet\u001b[38;5;241m.\u001b[39mnamed_modules\n\u001b[1;32m      2\u001b[0m layer_names\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipe' is not defined"
     ]
    }
   ],
   "source": [
    "pipe.unet.named_modules\n",
    "layer_names\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "photomaker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
