defaults:
  - trainer: photomaker_lora
  - model: photomaker_branched_lora
  - writer: console
  - transforms: only_instance
  - ddp: accelerate
  - lr_scheduler: custom_linear
  - pipeline: photomaker_branched
  - metrics: all_metrics
  # - datasets: all_datasets
  - datasets: all_datasets_local
  - dataloaders: all_dataloaders
  - _self_

optimizer:
  _convert_: partial
  _target_: torch.optim.AdamW
  weight_decay: 0

loss_function:
  _target_: src.loss.diffusion_loss.MaskedDiffusionLoss

lr_for_lora: 1e-5

train_dataset_name: "cosmic"
val_datasets_names: ["manual_val"]

inference_metrics: ["clip_ts", "id_sim_best"]

validation_debug_timing: false

validation_args:
  negative_prompt: "lowres, text, error, cropped, worst quality, low quality, jpeg artifacts, signature, watermark, blurry"
  debug_dir: hm_debug_orig
  num_images_per_prompt: 5
  num_inference_steps: 50
  guidance_scale: 5
  height: 1024
  width: 1024
  target_size: [1024, 1024]
  original_size: [1024, 1024]
  crops_coords_top_left: [0, 0]
  use_branched_attention: true
  face_embed_strategy: ${pipeline.face_embed_strategy}
  photomaker_start_step: ${pipeline.photomaker_start_step}
  merge_start_step: ${pipeline.merge_start_step}
  branched_attn_start_step: ${pipeline.branched_attn_start_step}
  branched_start_mode: ${pipeline.branched_start_mode}
